{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CeHaga-UFRJ/bmt-ufrj-2024.P1/blob/main/BMT_Trab1_Carlos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G-UpzubZUYC"
      },
      "source": [
        "# Busca e Mineração de Texto - 2024.1\n",
        "Carlos Bravo - 124066176"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeQ_qhv3Zsl_"
      },
      "source": [
        "## Bibliotecas e Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7809hp2Nsf1v",
        "outputId": "0877ed5d-c574-4525-fb16-59bb4ba08d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n"
          ]
        }
      ],
      "source": [
        "![ -d repo ] || git clone https://github.com/CeHaga-UFRJ/bmt-ufrj-2024.P1.git repo\n",
        "![ -d data ] || mv repo/data data\n",
        "![ -d config ] || mv repo/config config\n",
        "![ -d results ] || mkdir results\n",
        "\n",
        "!pip3 install nltk\n",
        "!pip3 install lxml\n",
        "!pip3 install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCwFUQ8-T5Vp",
        "outputId": "ed26027b-f018-4e91-bc34-e3c42434cc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from lxml import etree\n",
        "from unidecode import unidecode\n",
        "from math import log, sqrt\n",
        "import logging\n",
        "import time\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.DEBUG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KbaJSH8ktQkd"
      },
      "outputs": [],
      "source": [
        "def get_xml_root(file_name):\n",
        "    tree = etree.parse(file_name)\n",
        "    return tree.getroot()\n",
        "\n",
        "def text_normalize(text):\n",
        "    # Uppercase\n",
        "    text = text.upper()\n",
        "\n",
        "    # Remove accents\n",
        "    text = unidecode(text)\n",
        "\n",
        "    # Remove trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove semi-colons\n",
        "    text = text.replace(\";\", \"\")\n",
        "\n",
        "    # Remove newlines\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Change multiple spaces to single space\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "def text_to_tokens(text):\n",
        "    # Tokenize text\n",
        "    text = word_tokenize(text)\n",
        "\n",
        "    # Remove small words\n",
        "    text = [word for word in text if len(word) > 1]\n",
        "\n",
        "    # Remove words with numbers\n",
        "    text = [word for word in text if not any(char.isdigit() for char in word)]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "    text = [word for word in text if word.lower() not in stopwords]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = nltk.stem.PorterStemmer()\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "\n",
        "    # Uppercase\n",
        "    text = [word.upper() for word in text]\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEPTRNgmZxEv"
      },
      "source": [
        "## Processador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5Uh8t06KZyR-"
      },
      "outputs": [],
      "source": [
        "class Processer:\n",
        "  def __init__(self, cfg_file):\n",
        "    self.xml_root = None\n",
        "    self.query_file = \"\"\n",
        "    self.expected_file = \"\"\n",
        "    self._read_cfg(cfg_file)\n",
        "\n",
        "  def _read_cfg(self, cfg_file):\n",
        "    logging.info(f'Lendo arquivo de configuração {cfg_file}')\n",
        "    start_time = time.time()\n",
        "    with open(cfg_file, 'r') as config:\n",
        "      for line in config:\n",
        "        command, file_name = line.replace(\"\\n\",\"\").split(\"=\")\n",
        "        if(command == 'LEIA'):\n",
        "          root = get_xml_root(file_name)\n",
        "          self.xml_root = root\n",
        "        elif(command == 'CONSULTAS'):\n",
        "          self.query_file = file_name\n",
        "        elif(command == 'ESPERADOS'):\n",
        "          self.expected_file = file_name\n",
        "    logging.info(f'Arquivo de configuração lido em {time.time() - start_time:.6f} segundos')\n",
        "\n",
        "  def process(self):\n",
        "    self._create_query_file()\n",
        "    self._create_expected_file()\n",
        "\n",
        "  def _create_query_file(self):\n",
        "    logging.info('Criando arquivo de consultas')\n",
        "    start_time = time.time()\n",
        "    query_total = 0\n",
        "    with open(self.query_file, 'w') as query_file:\n",
        "      # Write headers\n",
        "      query_file.write(\"QueryNumber;QueryText\\n\")\n",
        "\n",
        "      # Process each query\n",
        "      for query in self.xml_root.iter('QUERY'):\n",
        "        query_number, query_text = self._get_query_data(query)\n",
        "        query_file.write(f\"{query_number};{query_text}\\n\")\n",
        "        query_total += 1\n",
        "\n",
        "    logging.info(f'Arquivo com {query_total} consultas criado em {time.time() - start_time:.6f} segundos')\n",
        "\n",
        "  def _create_expected_file(self):\n",
        "    logging.info('Criando arquivo de esperados')\n",
        "    start_time = time.time()\n",
        "    expected_total = 0\n",
        "    with open(self.expected_file, 'w') as expected_file:\n",
        "      # Write headers\n",
        "      expected_file.write(\"QueryNumber;DocNumber;DocVotes\\n\")\n",
        "\n",
        "      # Process each query\n",
        "      for query in self.xml_root.iter('QUERY'):\n",
        "        for query_number, doc_num, doc_votes in self._get_expected_data(query):\n",
        "          expected_file.write(f\"{query_number};{doc_num};{doc_votes}\\n\")\n",
        "          expected_total += 1\n",
        "\n",
        "    logging.info(f'Arquivo com {expected_total} esperados criado em {time.time() - start_time:.6f} segundos')\n",
        "\n",
        "  def _get_query_data(self, query):\n",
        "    # Get query number and text\n",
        "    query_number = query.find('QueryNumber').text\n",
        "    query_text = query.find('QueryText').text\n",
        "    query_text = text_normalize(query_text)\n",
        "\n",
        "    return query_number, query_text\n",
        "\n",
        "  def _get_expected_data(self, query):\n",
        "    query_number = query.find('QueryNumber').text\n",
        "    records = query.find('Records')\n",
        "    for item in records.iter('Item'):\n",
        "      doc_num = item.text\n",
        "      score = item.get('score')\n",
        "      doc_votes = len(score) - score.count('0')\n",
        "      yield query_number, doc_num, doc_votes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3hkzPrAtQkg",
        "outputId": "35bfbca8-8fe7-4c7d-898a-05005e98b735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Lendo arquivo de configuração config/pc.cfg\n",
            "INFO:root:Arquivo de configuração lido em 0.009738 segundos\n",
            "INFO:root:Criando arquivo de consultas\n",
            "INFO:root:Arquivo com 99 consultas criado em 0.003070 segundos\n",
            "INFO:root:Criando arquivo de esperados\n",
            "INFO:root:Arquivo com 4820 esperados criado em 0.007703 segundos\n"
          ]
        }
      ],
      "source": [
        "processer = Processer('config/pc.cfg')\n",
        "processer.process()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgJUj6pdtQkh"
      },
      "source": [
        "## Gerador de Lista Invertida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DaVvjS6WWvC9"
      },
      "outputs": [],
      "source": [
        "class InvertedList:\n",
        "  def __init__(self, cfg_file):\n",
        "    self.inverted_list = {}\n",
        "    self.xml_root = []\n",
        "    self.output_file = \"\"\n",
        "    self._read_cfg(cfg_file)\n",
        "\n",
        "  def invert(self):\n",
        "    logging.info('Criando lista invertida')\n",
        "    start_time = time.time()\n",
        "    # Process each XML file\n",
        "    for root in self.xml_root:\n",
        "      for record in root.iter('RECORD'):\n",
        "        # Get abstract from record\n",
        "        abstract = self._get_abstract_from_record(record)\n",
        "        if not abstract: continue\n",
        "\n",
        "        # Tokenize abstract\n",
        "        words = text_normalize(abstract)\n",
        "        words = text_to_tokens(words)\n",
        "\n",
        "        # Get document number\n",
        "        doc_num = record.find('RECORDNUM').text.strip()\n",
        "\n",
        "        # Create inverted list\n",
        "        for word in words:\n",
        "          if word not in self.inverted_list:\n",
        "            self.inverted_list[word] = []\n",
        "          self.inverted_list[word].append(doc_num)\n",
        "\n",
        "    # Write inverted list to file\n",
        "    with open(self.output_file, 'w') as output_file:\n",
        "      for word, doc_list in self.inverted_list.items():\n",
        "        output_file.write(f\"{word};{doc_list}\\n\")\n",
        "\n",
        "    logging.info(f'Lista invertida criada em {time.time() - start_time:.6f} segundos')\n",
        "\n",
        "  def _read_cfg(self, cfg_file):\n",
        "    logging.info(f'Lendo arquivo de configuração {cfg_file}')\n",
        "    start_time = time.time()\n",
        "    with open(cfg_file, 'r') as config:\n",
        "      for line in config:\n",
        "        command, file_name = line.replace(\"\\n\",\"\").split(\"=\")\n",
        "        if(command == 'LEIA'):\n",
        "          root = get_xml_root(file_name)\n",
        "          self.xml_root.append(root)\n",
        "        elif(command == 'ESCREVA'):\n",
        "          self.output_file = file_name\n",
        "    logging.info(f'Arquivo de configuração lido em {time.time() - start_time:.6f} segundos')\n",
        "\n",
        "  def _get_abstract_from_record(self, record):\n",
        "    abstract_element = record.find('ABSTRACT')\n",
        "    if abstract_element is not None:\n",
        "      return abstract_element.text.strip()\n",
        "\n",
        "    abstract_element = record.find('EXTRACT')\n",
        "    if abstract_element is not None:\n",
        "      return abstract_element.text.strip()\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcj5ZV1MWOfQ",
        "outputId": "d27fd1d0-ad70-44f1-b887-c1400616ff97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Lendo arquivo de configuração config/gli.cfg\n",
            "INFO:root:Arquivo de configuração lido em 0.150124 segundos\n",
            "INFO:root:Criando lista invertida\n",
            "INFO:root:Lista invertida criada em 5.561782 segundos\n"
          ]
        }
      ],
      "source": [
        "inverted_list = InvertedList('config/gli.cfg')\n",
        "inverted_list.invert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG0396bVtQkk"
      },
      "source": [
        "## Indexador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ccOQE2_7tQkk"
      },
      "outputs": [],
      "source": [
        "class Indexer:\n",
        "    def __init__(self, cfg_file):\n",
        "        self.word_freq = {}\n",
        "        self.max_freq_doc = {}\n",
        "        self.inverted_list_file = \"\"\n",
        "        self.output_model_file = \"\"\n",
        "        self._read_cfg(cfg_file)\n",
        "\n",
        "    def index(self):\n",
        "        self._read_inverted_list(self.inverted_list_file)\n",
        "        self._create_model()\n",
        "\n",
        "    def _read_inverted_list(self, inverted_list_file):\n",
        "        logging.info(f'Lendo lista invertida {inverted_list_file}')\n",
        "        start_time = time.time()\n",
        "        with open(self.inverted_list_file, 'r') as inverted_list_file:\n",
        "            for line in inverted_list_file:\n",
        "                word, doc_list = line.replace(\"\\n\",\"\").split(\";\")\n",
        "                doc_list = doc_list.replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\").split(\",\")\n",
        "                for doc in doc_list:\n",
        "                    if word not in self.word_freq:\n",
        "                        self.word_freq[word] = {}\n",
        "                    if doc not in self.word_freq[word]:\n",
        "                        self.word_freq[word][doc] = 0\n",
        "                    self.word_freq[word][doc] += 1\n",
        "                    if doc not in self.max_freq_doc:\n",
        "                        self.max_freq_doc[doc] = 0\n",
        "                    if self.word_freq[word][doc] > self.max_freq_doc[doc]:\n",
        "                        self.max_freq_doc[doc] = self.word_freq[word][doc]\n",
        "        logging.info(f'Lista invertida lida em {time.time() - start_time:.6f} segundos')\n",
        "\n",
        "    def _create_model(self):\n",
        "        logging.info(f'Criando modelo {self.output_model_file}')\n",
        "        start_time = time.time()\n",
        "        with open(self.output_model_file, 'w') as output_file:\n",
        "            for word in self.word_freq:\n",
        "                idf = self._get_idf(word)\n",
        "                for doc in self.word_freq[word]:\n",
        "                    tf = self._get_tf(word, doc)\n",
        "                    tf_idf = tf * idf\n",
        "                    output_file.write(f\"{word};{doc};{tf_idf}\\n\")\n",
        "        logging.info(f'Modelo criado em {time.time() - start_time:.6f} segundos')\n",
        "\n",
        "    def _read_cfg(self, cfg_file):\n",
        "        logging.info(f'Lendo arquivo de configuração {cfg_file}')\n",
        "        start_time = time.time()\n",
        "        with open(cfg_file, 'r') as config:\n",
        "            for line in config:\n",
        "                command, file_name = line.replace(\"\\n\",\"\").split(\"=\")\n",
        "                if(command == 'LEIA'):\n",
        "                    self.inverted_list_file = file_name\n",
        "                elif(command == 'ESCREVA'):\n",
        "                    self.output_model_file = file_name\n",
        "        logging.info(f'Arquivo de configuração lido em {time.time() - start_time:.6f} segundos')\n",
        "\n",
        "    def _get_tf(self, word, doc):\n",
        "        if word not in self.word_freq and doc in self.word_freq[word]:\n",
        "            return 0\n",
        "        tf = self.word_freq[word][doc]\n",
        "        return tf / self.max_freq_doc[doc]\n",
        "\n",
        "    def _get_n(self):\n",
        "        return len(self.word_freq)\n",
        "\n",
        "    def _get_ni(self, word):\n",
        "        if word in self.word_freq:\n",
        "            return len(self.word_freq[word])\n",
        "        return 0\n",
        "\n",
        "    def _get_idf(self, word):\n",
        "        return log(self._get_n() / self._get_ni(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyESj6SOtQkk",
        "outputId": "dc313aee-5113-4a24-8ac9-8e6293b40d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Lendo arquivo de configuração config/index.cfg\n",
            "INFO:root:Arquivo de configuração lido em 0.000151 segundos\n",
            "INFO:root:Lendo lista invertida results/inverted_list.csv\n",
            "INFO:root:Lista invertida lida em 0.250247 segundos\n",
            "INFO:root:Criando modelo results/model.csv\n",
            "INFO:root:Modelo criado em 0.113286 segundos\n"
          ]
        }
      ],
      "source": [
        "indexer = Indexer('config/index.cfg')\n",
        "indexer.index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z10OhQsMtQkl"
      },
      "source": [
        "## Buscador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Tm4wqZqYtQkl"
      },
      "outputs": [],
      "source": [
        "class Searcher:\n",
        "    def __init__(self, cfg_file):\n",
        "        self.model_file = \"\"\n",
        "        self.query_file = \"\"\n",
        "        self.output_file = \"\"\n",
        "\n",
        "        self._read_cfg(cfg_file)\n",
        "\n",
        "        self._get_model()\n",
        "        self._get_queries()\n",
        "\n",
        "    def search(self):\n",
        "        logging.info('Iniciando busca')\n",
        "        start_time = time.time()\n",
        "        with open(self.output_file, 'w') as output_file:\n",
        "            similarities = {}\n",
        "            for query_number, query_text in self.queries.items():\n",
        "                query_vector = { word: 1 for word in query_text }\n",
        "                doc_vectors = {}\n",
        "                for word in query_text:\n",
        "                    if word in self.model:\n",
        "                        for doc in self.model[word]:\n",
        "                            if doc not in doc_vectors:\n",
        "                                doc_vectors[doc] = {}\n",
        "                            doc_vectors[doc][word] = self.model[word][doc]\n",
        "                similarities[query_number] = {}\n",
        "                for doc in doc_vectors:\n",
        "                    similarities[query_number][doc] = self._cosine_similarity(query_vector, doc_vectors[doc])\n",
        "            for query_number in similarities:\n",
        "                sorted_similarities = sorted(similarities[query_number].items(), key=lambda x: x[1], reverse=True)\n",
        "                i = 1\n",
        "                for doc, similarity in sorted_similarities:\n",
        "                    output_file.write(f\"{query_number};[{i},{doc},{similarity}]\\n\")\n",
        "                    i += 1\n",
        "        logging.info(f'Busca finalizada em {time.time() - start_time:.6f} segundos')\n",
        "\n",
        "    def _cosine_similarity(self, query_vector, doc_vector):\n",
        "        dot_product = 0\n",
        "        for word in query_vector:\n",
        "            if word in doc_vector:\n",
        "                dot_product += query_vector[word] * doc_vector[word]\n",
        "        query_norm = sqrt(sum([value**2 for value in query_vector.values()]))\n",
        "        doc_norm = sqrt(sum([value**2 for value in doc_vector.values()]))\n",
        "        return dot_product / (query_norm * doc_norm)\n",
        "\n",
        "    def _read_cfg(self, cfg_file):\n",
        "        logging.info(f'Lendo arquivo de configuração {cfg_file}')\n",
        "        start_time = time.time()\n",
        "        with open(cfg_file, 'r') as config:\n",
        "            for line in config:\n",
        "                command, file_name = line.replace(\"\\n\",\"\").split(\"=\")\n",
        "                if(command == 'MODELO'):\n",
        "                    self.model_file = file_name\n",
        "                elif(command == 'CONSULTAS'):\n",
        "                    self.query_file = file_name\n",
        "                elif(command == 'RESULTADOS'):\n",
        "                    self.output_file = file_name\n",
        "        logging.info(f'Arquivo de configuração lido em {time.time() - start_time:.6f} segundos')\n",
        "\n",
        "    def _get_model(self):\n",
        "        logging.info(f'Lendo modelo {self.model_file}')\n",
        "        start_time = time.time()\n",
        "        self.model = {}\n",
        "        with open(self.model_file, 'r') as model_file:\n",
        "            for line in model_file:\n",
        "                word, doc, tf_idf = line.replace(\"\\n\",\"\").split(\";\")\n",
        "                if word not in self.model:\n",
        "                    self.model[word] = {}\n",
        "                self.model[word][doc] = float(tf_idf)\n",
        "        logging.info(f'Modelo lido em {time.time() - start_time:.6f} segundos')\n",
        "\n",
        "    def _get_queries(self):\n",
        "        logging.info(f'Lendo consultas {self.query_file}')\n",
        "        start_time = time.time()\n",
        "        query_total = 0\n",
        "        self.queries = {}\n",
        "        with open(self.query_file, 'r') as query_file:\n",
        "            next(query_file)\n",
        "            for line in query_file:\n",
        "                query_number, query_text = line.replace(\"\\n\",\"\").split(\";\")\n",
        "                query_text = text_normalize(query_text)\n",
        "                query_text = text_to_tokens(query_text)\n",
        "                self.queries[query_number] = query_text\n",
        "                query_total += 1\n",
        "        logging.info(f'Consultas ({query_total}) lidas em {time.time() - start_time:.6f} segundos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAcNYWectQkm",
        "outputId": "e688f65b-972f-45f3-cfe0-1c12be3382c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Lendo arquivo de configuração config/busca.cfg\n",
            "INFO:root:Arquivo de configuração lido em 0.000161 segundos\n",
            "INFO:root:Lendo modelo results/model.csv\n",
            "INFO:root:Modelo lido em 0.088620 segundos\n",
            "INFO:root:Lendo consultas results/queries.csv\n",
            "INFO:root:Consultas (99) lidas em 0.054731 segundos\n",
            "INFO:root:Iniciando busca\n",
            "INFO:root:Busca finalizada em 0.572022 segundos\n"
          ]
        }
      ],
      "source": [
        "searcher = Searcher('config/busca.cfg')\n",
        "searcher.search()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}