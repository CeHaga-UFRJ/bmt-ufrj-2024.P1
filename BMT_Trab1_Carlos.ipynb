{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CeHaga-UFRJ/bmt-ufrj-2024.P1/blob/main/BMT_Trab1_Carlos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G-UpzubZUYC"
      },
      "source": [
        "# Busca e Mineração de Texto - 2024.1\n",
        "Carlos Bravo - 124066176"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeQ_qhv3Zsl_"
      },
      "source": [
        "## Bibliotecas e Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7809hp2Nsf1v",
        "outputId": "9210a438-f566-4227-9d6b-fa25709d3106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'repo' already exists and is not an empty directory.\n",
            "mv: cannot stat 'repo/data': No such file or directory\n",
            "mv: cannot stat 'repo/config': No such file or directory\n",
            "mkdir: cannot create directory ‘results’: File exists\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/CeHaga-UFRJ/bmt-ufrj-2024.P1.git repo\n",
        "!mv repo/data data\n",
        "!mv repo/config config\n",
        "!mkdir results\n",
        "\n",
        "!pip3 install nltk\n",
        "!pip3 install lxml\n",
        "!pip3 install unidecode\n",
        "!pip3 install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCwFUQ8-T5Vp",
        "outputId": "fd0b16b6-5c0e-498d-f697-6b24f838d2fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from lxml import etree\n",
        "from unidecode import unidecode\n",
        "from math import log, sqrt\n",
        "import numpy as np\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KbaJSH8ktQkd"
      },
      "outputs": [],
      "source": [
        "def get_xml_root(file_name):\n",
        "    tree = etree.parse(file_name)\n",
        "    return tree.getroot()\n",
        "\n",
        "def text_normalize(text):\n",
        "    # Uppercase\n",
        "    text = text.upper()\n",
        "\n",
        "    # Remove accents\n",
        "    text = unidecode(text)\n",
        "\n",
        "    # Remove trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove semi-colons\n",
        "    text = text.replace(\";\", \"\")\n",
        "\n",
        "    # Remove newlines\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Change multiple spaces to single space\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "def text_to_tokens(text):\n",
        "    # Tokenize text\n",
        "    text = word_tokenize(text)\n",
        "\n",
        "    # Remove small words\n",
        "    text = [word for word in text if len(word) > 1]\n",
        "\n",
        "    # Remove words with numbers\n",
        "    text = [word for word in text if not any(char.isdigit() for char in word)]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "    text = [word for word in text if word.lower() not in stopwords]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = nltk.stem.PorterStemmer()\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "\n",
        "    # Uppercase\n",
        "    text = [word.upper() for word in text]\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEPTRNgmZxEv"
      },
      "source": [
        "## Processador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5Uh8t06KZyR-"
      },
      "outputs": [],
      "source": [
        "class Processer:\n",
        "  def __init__(self, cfg_file):\n",
        "    self.xml_root = None\n",
        "    self.query_file = \"\"\n",
        "    self.expected_file = \"\"\n",
        "    self._read_cfg(cfg_file)\n",
        "\n",
        "  def _read_cfg(self, cfg_file):\n",
        "    with open(cfg_file, 'r') as config:\n",
        "      for line in config:\n",
        "        command, file_name = line.replace(\"\\n\",\"\").split(\"=\")\n",
        "        if(command == 'LEIA'):\n",
        "          root = get_xml_root(file_name)\n",
        "          self.xml_root = root\n",
        "        elif(command == 'CONSULTAS'):\n",
        "          self.query_file = file_name\n",
        "        elif(command == 'ESPERADOS'):\n",
        "          self.expected_file = file_name\n",
        "\n",
        "  def process(self):\n",
        "    self._create_query_file()\n",
        "    self._create_expected_file()\n",
        "\n",
        "  def _create_query_file(self):\n",
        "    print('Criando arquivo de consultas...')\n",
        "    with open(self.query_file, 'w') as query_file:\n",
        "      # Write headers\n",
        "      query_file.write(\"QueryNumber;QueryText\\n\")\n",
        "\n",
        "      # Process each query\n",
        "      for query in self.xml_root.iter('QUERY'):\n",
        "        query_number, query_text = self._get_query_data(query)\n",
        "        query_file.write(f\"{query_number};{query_text}\\n\")\n",
        "\n",
        "    print('Arquivo de consultas criado\\n')\n",
        "\n",
        "  def _create_expected_file(self):\n",
        "    print('Criando arquivo de esperados...')\n",
        "    with open(self.expected_file, 'w') as expected_file:\n",
        "      # Write headers\n",
        "      expected_file.write(\"QueryNumber;DocNumber;DocVotes\\n\")\n",
        "\n",
        "      # Process each query\n",
        "      for query in self.xml_root.iter('QUERY'):\n",
        "        for query_number, doc_num, doc_votes in self._get_expected_data(query):\n",
        "          expected_file.write(f\"{query_number};{doc_num};{doc_votes}\\n\")\n",
        "\n",
        "    print('Arquivo de esperados criado\\n')\n",
        "\n",
        "  def _get_query_data(self, query):\n",
        "    # Get query number and text\n",
        "    query_number = query.find('QueryNumber').text\n",
        "    query_text = query.find('QueryText').text\n",
        "    query_text = text_normalize(query_text)\n",
        "\n",
        "    return query_number, query_text\n",
        "\n",
        "  def _get_expected_data(self, query):\n",
        "    query_number = query.find('QueryNumber').text\n",
        "    records = query.find('Records')\n",
        "    for item in records.iter('Item'):\n",
        "      doc_num = item.text\n",
        "      score = item.get('score')\n",
        "      doc_votes = len(score) - score.count('0')\n",
        "      yield query_number, doc_num, doc_votes\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3hkzPrAtQkg",
        "outputId": "9233717e-96aa-4408-bb3c-9196fdf15e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Criando arquivo de consultas...\n",
            "Arquivo de consultas criado\n",
            "\n",
            "Criando arquivo de esperados...\n",
            "Arquivo de esperados criado\n",
            "\n"
          ]
        }
      ],
      "source": [
        "processer = Processer('config/pc.cfg')\n",
        "processer.process()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgJUj6pdtQkh"
      },
      "source": [
        "## Gerador de Lista Invertida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DaVvjS6WWvC9"
      },
      "outputs": [],
      "source": [
        "class InvertedList:\n",
        "  def __init__(self, cfg_file):\n",
        "    self.inverted_list = {}\n",
        "    self.xml_root = []\n",
        "    self.output_file = \"\"\n",
        "    self._read_cfg(cfg_file)\n",
        "\n",
        "  def invert(self):\n",
        "    print('Criando lista invertida...')\n",
        "    # Process each XML file\n",
        "    for root in self.xml_root:\n",
        "      for record in root.iter('RECORD'):\n",
        "        # Get abstract from record\n",
        "        abstract = self._get_abstract_from_record(record)\n",
        "        if not abstract: continue\n",
        "\n",
        "        # Tokenize abstract\n",
        "        words = text_normalize(abstract)\n",
        "        words = text_to_tokens(words)\n",
        "\n",
        "        # Get document number\n",
        "        doc_num = record.find('RECORDNUM').text.strip()\n",
        "\n",
        "        # Create inverted list\n",
        "        for word in words:\n",
        "          if word not in self.inverted_list:\n",
        "            self.inverted_list[word] = []\n",
        "          self.inverted_list[word].append(doc_num)\n",
        "\n",
        "    # Write inverted list to file\n",
        "    with open(self.output_file, 'w') as output_file:\n",
        "      for word, doc_list in self.inverted_list.items():\n",
        "        output_file.write(f\"{word};{doc_list}\\n\")\n",
        "\n",
        "    print('Lista invertida criada\\n')\n",
        "\n",
        "  def _read_cfg(self, cfg_file):\n",
        "    with open(cfg_file, 'r') as config:\n",
        "      for line in config:\n",
        "        command, file_name = line.replace(\"\\n\",\"\").split(\"=\")\n",
        "        if(command == 'LEIA'):\n",
        "          root = get_xml_root(file_name)\n",
        "          self.xml_root.append(root)\n",
        "        elif(command == 'ESCREVA'):\n",
        "          self.output_file = file_name\n",
        "\n",
        "  def _get_abstract_from_record(self, record):\n",
        "    abstract_element = record.find('ABSTRACT')\n",
        "    if abstract_element is not None:\n",
        "      return abstract_element.text.strip()\n",
        "\n",
        "    abstract_element = record.find('EXTRACT')\n",
        "    if abstract_element is not None:\n",
        "      return abstract_element.text.strip()\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcj5ZV1MWOfQ",
        "outputId": "1de33250-9fa4-4b70-cc2f-89c0ae396edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Criando lista invertida...\n",
            "Lista invertida criada\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inverted_list = InvertedList('config/gli.cfg')\n",
        "inverted_list.invert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG0396bVtQkk"
      },
      "source": [
        "## Indexador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ccOQE2_7tQkk"
      },
      "outputs": [],
      "source": [
        "class Indexer:\n",
        "    def __init__(self, cfg_file):\n",
        "        self.word_freq = {}\n",
        "        self.max_freq_doc = {}\n",
        "        self.inverted_list_file = \"\"\n",
        "        self.output_model_file = \"\"\n",
        "        self._read_cfg(cfg_file)\n",
        "\n",
        "    def index(self):\n",
        "        print('Indexando...')\n",
        "        self._read_inverted_list(self.inverted_list_file)\n",
        "        self._create_model()\n",
        "        print('Indexação concluída\\n')\n",
        "\n",
        "    def _read_inverted_list(self, inverted_list_file):\n",
        "        with open(self.inverted_list_file, 'r') as inverted_list_file:\n",
        "            for line in inverted_list_file:\n",
        "                word, doc_list = line.replace(\"\\n\",\"\").split(\";\")\n",
        "                doc_list = doc_list.replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\").split(\",\")\n",
        "                for doc in doc_list:\n",
        "                    if word not in self.word_freq:\n",
        "                        self.word_freq[word] = {}\n",
        "                    if doc not in self.word_freq[word]:\n",
        "                        self.word_freq[word][doc] = 0\n",
        "                    self.word_freq[word][doc] += 1\n",
        "                    if doc not in self.max_freq_doc:\n",
        "                        self.max_freq_doc[doc] = 0\n",
        "                    if self.word_freq[word][doc] > self.max_freq_doc[doc]:\n",
        "                        self.max_freq_doc[doc] = self.word_freq[word][doc]\n",
        "\n",
        "    def _create_model(self):\n",
        "        with open(self.output_model_file, 'w') as output_file:\n",
        "            for word in self.word_freq:\n",
        "                idf = self._get_idf(word)\n",
        "                for doc in self.word_freq[word]:\n",
        "                    tf = self._get_tf(word, doc)\n",
        "                    tf_idf = tf * idf\n",
        "                    output_file.write(f\"{word};{doc};{tf_idf}\\n\")\n",
        "\n",
        "    def _read_cfg(self, cfg_file):\n",
        "        with open(cfg_file, 'r') as config:\n",
        "            for line in config:\n",
        "                command, file_name = line.replace(\"\\n\",\"\").split(\"=\")\n",
        "                if(command == 'LEIA'):\n",
        "                    self.inverted_list_file = file_name\n",
        "                elif(command == 'ESCREVA'):\n",
        "                    self.output_model_file = file_name\n",
        "\n",
        "    def _get_tf(self, word, doc):\n",
        "        if word not in self.word_freq and doc in self.word_freq[word]:\n",
        "            return 0\n",
        "        tf = self.word_freq[word][doc]\n",
        "        return tf / self.max_freq_doc[doc]\n",
        "\n",
        "    def _get_n(self):\n",
        "        return len(self.word_freq)\n",
        "\n",
        "    def _get_ni(self, word):\n",
        "        if word in self.word_freq:\n",
        "            return len(self.word_freq[word])\n",
        "        return 0\n",
        "\n",
        "    def _get_idf(self, word):\n",
        "        return log(self._get_n() / self._get_ni(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyESj6SOtQkk",
        "outputId": "d550e3c8-10f2-4cd2-9702-7824a2200dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexando...\n",
            "Indexação concluída\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indexer = Indexer('config/index.cfg')\n",
        "indexer.index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z10OhQsMtQkl"
      },
      "source": [
        "## Buscador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Tm4wqZqYtQkl"
      },
      "outputs": [],
      "source": [
        "class Searcher:\n",
        "    def __init__(self, cfg_file):\n",
        "        self.model_file = \"\"\n",
        "        self.query_file = \"\"\n",
        "        self.output_file = \"\"\n",
        "\n",
        "        self._read_cfg(cfg_file)\n",
        "\n",
        "        self._get_model()\n",
        "        self._get_queries()\n",
        "\n",
        "    def search(self):\n",
        "        print('Buscando...')\n",
        "        with open(self.output_file, 'w') as output_file:\n",
        "            similarities = {}\n",
        "            for query_number, query_text in self.queries.items():\n",
        "                query_vector = { word: 1 for word in query_text }\n",
        "                doc_vectors = {}\n",
        "                for word in query_text:\n",
        "                    if word in self.model:\n",
        "                        for doc in self.model[word]:\n",
        "                            if doc not in doc_vectors:\n",
        "                                doc_vectors[doc] = {}\n",
        "                            doc_vectors[doc][word] = self.model[word][doc]\n",
        "                similarities[query_number] = {}\n",
        "                for doc in doc_vectors:\n",
        "                    similarities[query_number][doc] = self._cosine_similarity(query_vector, doc_vectors[doc])\n",
        "            for query_number in similarities:\n",
        "                sorted_similarities = sorted(similarities[query_number].items(), key=lambda x: x[1], reverse=True)\n",
        "                i = 1\n",
        "                for doc, similarity in sorted_similarities:\n",
        "                    output_file.write(f\"{query_number};[{i},{doc},{similarity}]\\n\")\n",
        "                    i += 1\n",
        "        print('Busca concluída\\n')\n",
        "\n",
        "    def _cosine_similarity(self, query_vector, doc_vector):\n",
        "        dot_product = 0\n",
        "        for word in query_vector:\n",
        "            if word in doc_vector:\n",
        "                dot_product += query_vector[word] * doc_vector[word]\n",
        "        query_norm = sqrt(sum([value**2 for value in query_vector.values()]))\n",
        "        doc_norm = sqrt(sum([value**2 for value in doc_vector.values()]))\n",
        "        return dot_product / (query_norm * doc_norm)\n",
        "\n",
        "    def _read_cfg(self, cfg_file):\n",
        "        with open(cfg_file, 'r') as config:\n",
        "            for line in config:\n",
        "                command, file_name = line.replace(\"\\n\",\"\").split(\"=\")\n",
        "                if(command == 'MODELO'):\n",
        "                    self.model_file = file_name\n",
        "                elif(command == 'CONSULTAS'):\n",
        "                    self.query_file = file_name\n",
        "                elif(command == 'RESULTADOS'):\n",
        "                    self.output_file = file_name\n",
        "\n",
        "    def _get_model(self):\n",
        "        self.model = {}\n",
        "        with open(self.model_file, 'r') as model_file:\n",
        "            for line in model_file:\n",
        "                word, doc, tf_idf = line.replace(\"\\n\",\"\").split(\";\")\n",
        "                if word not in self.model:\n",
        "                    self.model[word] = {}\n",
        "                self.model[word][doc] = float(tf_idf)\n",
        "\n",
        "    def _get_queries(self):\n",
        "        self.queries = {}\n",
        "        with open(self.query_file, 'r') as query_file:\n",
        "            for line in query_file:\n",
        "                query_number, query_text = line.replace(\"\\n\",\"\").split(\";\")\n",
        "                query_text = text_normalize(query_text)\n",
        "                query_text = text_to_tokens(query_text)\n",
        "                self.queries[query_number] = query_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAcNYWectQkm",
        "outputId": "9cc9a862-b297-400b-b7bd-635482e8b6a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buscando...\n",
            "Busca concluída\n",
            "\n"
          ]
        }
      ],
      "source": [
        "searcher = Searcher('config/busca.cfg')\n",
        "searcher.search()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}